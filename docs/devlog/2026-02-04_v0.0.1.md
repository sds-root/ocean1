# 2026-02-04 (v0.0.1): LLM Chat Integration

## Summary
- Implemented real-time LLM chat in `/chat` using Vercel AI SDK v6.
- Connected Frontend (`apps/web`) to Backend (`apps/api`) via streaming endpoint.
- Supported both Google Gemini (Cloud) and Ollama (Local) providers.
- Improved process management with `concurrently`.

## Files Changed
- `apps/api/src/index.ts` - Added `POST /api/chat` with streaming support.
- `apps/web/src/routes/chat.tsx` - Refactored to use `useChat` hook and `react-markdown`.
- `apps/api/.env` - Added configuration for LLM providers.
- `package.json` - Updated `dev` script to use `concurrently`.

## Key Technical Decisions
- **SDK v6 Upgrade**: Migrated to `ai` v6 and `@ai-sdk/react`.
- **Stream Protocol**: Configured `streamProtocol: 'text'` for compatibility.
- **Optimization**: Fixed unnecessary re-renders in Sidebar by moving component definition.

## Notes
- `bun install` required in both `apps/web` and `apps/api`.
- `react-markdown` added for message rendering.
